install.packages('caret')
install.packages("ROSE")
install.packages("DMwR2")
install.packages("randomForest")
#install.packages("klaR")
#install.packages("mlbench")
library(tidyr)
library(ggplot2)
library(scales)
library(googlesheets4)
library(dplyr)
library(caret)
library(ROSE)
library(e1071)
library(rpart)
library(Rborist)
library(pROC)
library(DMwR2)
library(randomForest)
library(klaR)
library(mlbench)

# Load csv file
# Reminder to upload the file whenever you start the session
#file_path <- "/content/clean_card_transdata.csv"
clean_data <- read.csv("clean_card_transdata.csv")

#To view the data table & structure
head(clean_data)
str(clean_data)

# Splitting data to X(Features) and y(label)
x <- clean_data %>% dplyr::select(-fraud)
y <- clean_data$fraud


#Dataset: Training 70%, Validation 20%, Test 10%

#Split the dataset into a training set and a test set
#list = FALSE: Returns a single vector of indices
train_index <- createDataPartition(y, p = 0.9, list = FALSE)
x_train <- x[train_index, ]
y_train <- y[train_index]
x_test <- x[-train_index, ]
y_test <- y[-train_index]



# Split above training set(90%) into a training set(70%) and a validation set(20%)
val_index <- createDataPartition(y_train, p = 0.20, list = FALSE)
x_validate <- x_train[val_index, ]
y_validate <- y_train[val_index]
x_train <- x_train[-val_index, ]
y_train <- y_train[-val_index]

#To check the dimension
cat("x training shape = ", dim(x_train), "\n")
cat("y training shape = ", length(y_train), "\n")
cat("x validation shape = ", dim(x_validate), "\n")
cat("y validation shape = ", length(y_validate), "\n")
cat("x test shape = ", dim(x_test), "\n")
cat("y test shape = ", length(y_test), "\n")



#Data scaling - Standard scaler

#Preprocessing -used to ensure that features are on a similar scale
#"center": Centering involves subtracting the mean of each variable (feature) from all data points.
#"scale": Scaling involves dividing each variable by its standard deviation.
scaler <- preProcess(x_train, method = c("center", "scale"))

#prediction
x_train <- predict(scaler, x_train)
x_validate <- predict(scaler, x_validate)
x_test <- predict(scaler, x_test)

#To view the data structure
str(x_train)
str(x_validate)
str(x_test)



#ROSE(Random Over-Sampling Example) Imbalance Technique

#Combine X_train and y_train into a data frame
data_train <- data.frame(cbind(x_train, y_train))
#str(data_train)
str(data_train)

#Perform over-sampling with ROSE
#y_train specifies that you want to oversample the minority class (y_train)
rose_result <- ROSE(y_train ~ ., data = data_train, seed=42)
#str(rose_result)

#Extract oversampled data
x_train_os <- rose_result$data[, -ncol(rose_result$data)]  # Exclude the last column (y_train)
y_train_os <- rose_result$data[, ncol(rose_result$data)]   # Extract the last column (y_train)

#Count of each unique value in y_train
table(y_train)
cat("\n")

#Count of each unique value in y_train after using ROSE
table(y_train_os)



# SMOTE Imbalance Technique
smote_result <- SMOTE(y_train ~ ., data = cbind(y_train, x_train), perc.over = 100, k = 5)

# Extract oversampled data
#x_train_os <- smote_result$SMOTE
#y_train_os <- smote_result$y_train

# Count of each unique value in y_train
#table(y_train)

# Count of each unique value in y_train after using SMOTE
#table(y_train_os)


#length(x_train_os)
#length(y_train_os)

#Logistic Regression

#Combine X_train_os and y_train_os into a data frame
data_train_os <- data.frame(cbind(x_train_os, y_train_os))

#Instantiate Logistic Regression Classifier
#Bionomial: binary classification
log_reg <- glm(y_train_os ~ ., data = data_train_os, family = 'binomial')

#Training Accuracy
predicted_probs <- predict(log_reg, data_train_os, type = "response")
predicted_labels <- ifelse(predicted_probs > 0.5, 1, 0)
training_accuracy <- mean(predicted_labels == y_train_os)

#Print the results
cat("Training Accuracy:", training_accuracy, "\n")



#Logistic Regression

#Make predictions on test set using Logistic Regression
y_pred <- predict(log_reg, newdata = x_test, type = "response")

#Convert predicted probabilities to class labels
y_pred <- ifelse(y_pred > 0.5, 1, 0) # binary classification task

#convert to factor
y_pred <- as.factor(y_pred)
y_test <- as.factor(y_test)

# for 0 
#Logistic Regression
#Confusion matrix
#data: The predicted labels or classifications generated by the model
#reference: The actual labels or true values
conf_matrix <- confusionMatrix(data = y_pred, reference = y_test)
print("Confusion Matrix:")
print(conf_matrix)


# Extract confusion matrix data
cm_data <- as.table(conf_matrix)

# Calculate recall, precision, and F values
recall <- recall(cm_data)
precision <- precision(cm_data)
f_value <- F_meas(cm_data)

# Print the results
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f_value))


#Logistic Regression
#Plot confusion Matrix
conf_matrix_df <- as.data.frame(as.table(conf_matrix))

ggplot(data = conf_matrix_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(name = "Frequency", low = "yellow", high = 'Red') +
  labs(x = "Predicted", y = "Actual", title = "Confusion Matrix") +
  theme_minimal()#Logistic Regression




# Specify the number of folds for cross-validation
num_folds <- 20
cv_model <- train(y_train_os ~ ., data = data_train_os, method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = num_folds)
)

# Print the cross-validated results
print(cv_model)

#creat ROC curve on test data 
#to measure the accuracy of a classification prediction
#showing the performance of a classification model at all classification thresholds
#gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red")
library(pROC)

roc_curve <- roc(y_pred, predict(cv_model, newdata = x_test, type = "raw"),plotit = FALSE)
plot(roc_curve)
# Add AUC (Area Under the Curve) to the plot
auc_value<- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), col = "blue", lty = 1, cex = 0.8)


#Decision tree
dec_tree<- rpart(y_train_os ~ ., data = data_train_os, method = "class" )

#Training Accuracy
predicted_probs1 <- predict(dec_tree, data_train_os, type = "class")
training_accuracy1 <- mean(predicted_probs1 == y_train_os)

#Print the results
cat("Training Accuracy:", training_accuracy1, "\n")


#Decision Tree

#Make predictions on test set using decision tress
y_pred1 <- predict(dec_tree, newdata = x_test, type = "prob")

#Convert predicted probabilities to class labels
y_pred1 <- ifelse(y_pred1 > 0.5, 1, 0) # binary classification task

#convert to factor
y_pred1 <- as.factor(y_pred1)
y_test <- as.factor(y_test)



#Decision Tress
#Confusion matrix
#data: The predicted labels or classifications generated by the model
#reference: The actual labels or true values
conf_matrix1 <- confusionMatrix(data = y_pred1, reference = y_test)
print("Confusion Matrix:")
print(conf_matrix1)


# Extract confusion matrix data
cm_data1 <- as.table(conf_matrix1)

# Calculate recall, precision, and F values
recall <- recall(cm_data1)
precision <- precision(cm_data1)
f_value <- F_meas(cm_data1)

# Print the results
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f_value))

#Decision Tress
#Plot confusion Matrix
conf_matrix_df1 <- as.data.frame(as.table(conf_matrix1))

ggplot(data = conf_matrix_df1, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(name = "Frequency", low = "yellow", high = 'Red') +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix") +
  theme_minimal()#Decision Tress



#Random Forest
rf_model<- randomForest(y_train_os ~ ., data = data_train_os, ntree = 100, importance = TRUE)

#Training Accuracy
predicted_probs2 <- predict(rf_model, data_train_os, type = "response")
predicted_labels2 <- ifelse(predicted_probs2 > 0.5, 1, 0)
training_accuracy2 <- mean(predicted_labels2 == y_train_os)

#Print the results
cat("Training Accuracy2:", training_accuracy2, "\n")

## Create a ROC curve
#roc_curve <- multiclass.roc(x_validate, predicted_probs2)
#plot(roc_curve, main = "ROC Curve for Random Forest",col = c("red", "green", "blue"), lwd = 2)

#auc_value <- auc(roc_curve)
#legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), col = "blue", lty = 1, cex = 0.8)
#Logistic Regression

#Make predictions on test set using random forest
y_pred2 <- predict(rf_model, newdata = x_test, type = "response")

#Convert predicted probabilities to class labels
y_pred2 <- ifelse(y_pred2 > 0.5, 1, 0) # binary classification task

#convert to factor
y_pred2 <- as.factor(y_pred2)
y_test <- as.factor(y_test)

conf_matrix2 <- confusionMatrix(data = y_pred2, reference = y_test)
print("Confusion Matrix:")
print(conf_matrix2)


# Extract confusion matrix data
cm_data2<- as.table(conf_matrix2)

# Calculate recall, precision, and F values
recall <- recall(cm_data2)
precision <- precision(cm_data2)
f_value <- F_meas(cm_data2)

# Print the results
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f_value))

#Random Forest
#Plot confusion Matrix
conf_matrix_df2<- as.data.frame(as.table(conf_matrix2))

ggplot(data = conf_matrix_df2, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(name = "Frequency", low = "yellow", high = 'Red') +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix") +
  theme_minimal()#random forest



#Naive Bayes
nb_model<- naiveBayes(y_train_os ~ ., data = data_train_os)

#Training Accuracy
predicted_probs3 <- predict(nb_model, data_train_os)
training_accuracy3 <- mean(predicted_probs3 == y_train_os)

#Print the results
cat("Training Accuracy:", training_accuracy3, "\n")

## Create a ROC curve
#roc_curve <- roc(test_data$fraud, test_predictions[, 2])
#plot(roc_curve, main = "ROC Curve for Random Forest",col = c("red", "green", "blue"), lwd = 2)

#auc_value <- auc(roc_curve)
#legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), col = "blue", lty = 1, cex = 0.8)
#Logistic Regression

#Make predictions on test set using naive bayes
y_pred3 <- predict(nb_model, newdata = x_test, type = "raw")



#Convert predicted probabilities to class labels
threshold <- 0.5
y_pred3 <- ifelse(y_pred3[, 2] >= threshold, 1, 0) # binary classification task


roc_curve <- roc(x_testl, y_pred3[, 2])
plot(roc_curve, main = "ROC Curve for Naive Bayes", col = "blue", lwd = 2)

#convert to factor
y_pred3 <- as.factor(y_pred3)
y_test <- as.factor(y_test)

conf_matrix3 <- confusionMatrix(data = y_pred3, reference = y_test)
print("Confusion Matrix:")
print(conf_matrix3)


# Extract confusion matrix data
cm_data3<- as.table(conf_matrix3)

# Calculate recall, precision, and F values
recall <- recall(cm_data3)
precision <- precision(cm_data3)
f_value <- F_meas(cm_data3)

# Print the results
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f_value))

#Naive bayes
#Plot confusion Matrix
conf_matrix_df3<- as.data.frame(as.table(conf_matrix3))

ggplot(data = conf_matrix_df3, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(name = "Frequency", low = "yellow", high = 'Red') +
  labs(x = "Predicted", y = "Actual", title = "Confusion Matrix") +
  theme_minimal()#naive bayes








#K-fold cross validation
# Specify the number of folds for cross-validation
num_folds <- 5

# Create a cross-validated decision tree model
cv_model <- train( y_train_os ~ ., data = data_train_os, method = "rpart", 
            trControl = trainControl(method = "cv", number = num_folds),
  tuneLength = 5  # Number of hyperparameter values to explore (adjust as needed)
)

print(cv_model)

# Make predictions on the test set
test_predictions <- predict(cv_model, newdata = x_test)

#convert to factor
test_predictions <- as.factor(test_predictions)
y_test <- as.factor(y_test)

# Evaluate the model
conf_matrix4 <- confusionMatrix(data = test_predictions, reference = y_test)
print("Confusion Matrix:")
print(conf_matrix4)

cm_data4<- as.table(conf_matrix4)
